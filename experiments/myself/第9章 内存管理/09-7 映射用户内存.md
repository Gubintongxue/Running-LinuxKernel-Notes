# 实验 9-7：映射用户内存

## 1．实验目的

​		映射用户内存用于把用户空间的虚拟内存空间传到内核空间。内核空间为其分配物理内存并建立相应的映射关系，并且锁住（pin）这些物理内存。这种方法在很多驱动程序中非常常见，比如在 camera 驱动的 V4L2 核心架构中可以使用用户空间内存类型（V4L2_MEMORY_ USERPTR）来分配物理内存，其驱动的实现使用的是 get_user_pages()函数。

​		本实验尝试使用 get_user_pages()函数来分配和锁住物理内存。

## 2．实验要求

​		1）编写一个简单的字符设备程序。使用 get_user_pages()函数为用户空间传递下来的虚拟地址空间分配和锁住物理内存。

​		2）写一个简单的用户空间的测试程序，来测试这个字符设备驱动。

## 3．实验步骤

## 下面是本实验的实验步骤。

### 启动 QEMU+runninglinuxkernel。

```
$ ./run_rlk_arm64.sh run
```



### 进入本实验的参考代码。

```
# cd /mnt/rlk_lab/rlk_basic/chapter_9_mm/lab7
```



### 编译内核模块。

```
benshushu:lab7_pin_page# make
make -C /lib/modules/`uname -r`/build 
M=/mnt/rlk_lab/rlk_basic/chapter_9_mm/lab7_pin_page modules;
make[1]: Entering directory '/usr/src/linux'
 CC [M] /mnt/rlk_lab/rlk_basic/chapter_9_mm/lab7_pin_page/mydev_pin_page.o
 LD [M] /mnt/rlk_lab/rlk_basic/chapter_9_mm/lab7_pin_page/mydevdemo-pinpage.o
 Building modules, stage 2.
 MODPOST 1 modules
 CC /mnt/rlk_lab/rlk_basic/chapter_9_mm/lab7_pin_page/mydevdemo-pinpage.mod.o
 LD [M] /mnt/rlk_lab/rlk_basic/chapter_9_mm/lab7_pin_page/mydevdemo-pinpage.ko
make[1]: Leaving directory '/usr/src/linux'
```

#### 安装内核模块。

```
benshushu:lab7_pin_page# insmod mydevdemo-pin-page.ko 
[ 1548.416556] succeeded register char device: my_demo_dev
benshushu:lab7_pin_page#
```

### 编译和运行 test_issue 程序。

```
benshushu:lab7_pin_page# gcc test_issue.c -o test_issue
benshushu:lab7_pin_page# ./test_issue 
[ 1591.642809] demodrv_open: major=10, minor=58
driver max buffer size=4096
[ 1591.650557] demodrv_read_write: len=4096, npage=1
[ 1591.651127] pin 1 pages from user done
[ 1591.651636] demodrv_read_write: write user buffer 4096 bytes done
[ 1591.651920] demodrv_write: write nbytes=4096 done at pos=0
[ 1591.652291] demodrv_read_write: len=4096, npage=1
[ 1591.652564] pin 1 pages from user done
[ 1591.652825] demodrv_read_write: read user buffer 4096 bytes done
[ 1591.653088] demodrv_read: read nbytes=4096 done at pos=0
buffer compare fail
free(): invalid pointer
Aborted
```

![image-20240924154013905](image/image-20240924154013905.png)

​		发现跑出错误了。“buffer compare fail”说明 buffer 读回来的数据和原始值不一样。这是为什么呢？难道是我们驱动代码有问题吗？

​		我们再运行 test_ok 测试程序，发现跑通了，log 里显示：data modify and compare succussful。

```
benshushu:lab7_pin_page# gcc test_ok.c -o test_ok
benshushu:lab7_pin_page# ./test_ok 
[ 1670.322993] demodrv_open: major=10, minor=58
driver max buffer size=4096
[ 1670.345312] demodrv_read_write: len=4096, npage=1
[ 1670.345715] pin 1 pages from user done
[ 1670.345911] demodrv_read_write: write user buffer 4096 bytes done
[ 1670.346247] demodrv_write: write nbytes=4096 done at pos=0
[ 1670.346608] demodrv_read_write: len=4096, npage=1
[ 1670.347133] pin 1 pages from user done
[ 1670.347385] demodrv_read_write: read user buffer 4096 bytes done
[ 1670.347670] demodrv_read: read nbytes=4096 done at pos=0
data modify and compare succussful
```

![image-20240924154043086](image/image-20240924154043086.png)

请读者思考，为什么跑 test_issue 程序会出现错误？

## 4．实验参考代码

驱动的参考代码如下。

```C
1 #include <linux/module.h>
2 #include <linux/fs.h>
3 #include <linux/uaccess.h>
4 #include <linux/init.h>
5 #include <linux/miscdevice.h>
6 #include <linux/device.h>
7 #include <linux/slab.h>
8 #include <linux/kfifo.h>
9 #include <linux/highmem.h>
10 
11 #define DEMO_NAME "my_demo_dev"
12 static struct device *mydemodrv_device;
13 
14 #define MYDEMO_READ 0
15 #define MYDEMO_WRITE 1
16 
17 /*virtual FIFO device's buffer*/
18 static char *device_buffer;
19 #define MAX_DEVICE_BUFFER_SIZE (1 * PAGE_SIZE)
20 
21 #define MYDEV_CMD_GET_BUFSIZE 1 /* defines our IOCTL cmd */
22 
23 static size_t
24 demodrv_read_write(void *buf, size_t len,
25 int rw)
26 {
27 int ret, npages, i;
28 struct page **pages;
29 struct mm_struct *mm = current->mm;
30 char *kmap_addr, *dev_buf;
31 size_t size = 0;
32 size_t count = 0;
33 
34 dev_buf = device_buffer;
35 
36 /* how mange pages? */
37 npages = DIV_ROUND_UP(len, PAGE_SIZE);
38 
39 printk("%s: len=%d, npage=%d\n", __func__, len, npages);
40 
41 pages = kmalloc(npages * sizeof(pages), GFP_KERNEL);
42 if (!pages) {
43 printk("alloc pages fail\n");
44 return -ENOMEM;
45 }
46 
47 down_read(&mm->mmap_sem);
48 
49 ret = get_user_pages_fast((unsigned long)buf, npages, 1, pages);
50 if (ret < npages) {
51 printk("pin page fail\n");
52 goto fail_pin_pages;
53 }
54 
55 up_read(&mm->mmap_sem);
56 
57 printk("pin %d pages from user done\n", npages);
58 
59 for (i = 0; i < npages; i++) {
60 kmap_addr = kmap(pages[i]);
61 //print_hex_dump_bytes("kmap:", DUMP_PREFIX_OFFSET, 
kmap_addr, PAGE_SIZE);
62 size = min_t(size_t, PAGE_SIZE, len);
63 switch(rw) {
64 case MYDEMO_READ:
65 memcpy(kmap_addr, dev_buf + PAGE_SIZE *i,
66 size);
67 //print_hex_dump_bytes("read:", DUMP_PREFIX_OFFSET, 
kmap_addr, size);
68 break;
69 case MYDEMO_WRITE:
70 memcpy(dev_buf + PAGE_SIZE*i, kmap_addr,
          71 size);
72 //print_hex_dump_bytes("write:", DUMP_PREFIX_OFFSET, 
dev_buf + PAGE_SIZE*i, size);
73 break;
74 default:
75 break;
76 }
77 put_page(pages[i]);
78 kunmap(pages[i]);
79 len -= size;
80 count += size;
81 }
82 
83 kfree(pages);
84 
85 printk("%s: %s user buffer %d bytes done\n", __func__, rw ? 
"write":"read", count);
86 
87 return count;
88 
89 fail_pin_pages:
90 up_read(&mm->mmap_sem);
91 for (i = 0; i < ret; i++)
92 put_page(pages[i]);
93 kfree(pages);
94 
95 return -EFAULT;
96 }
97 
98 static int demodrv_open(struct inode *inode, struct file *file)
99 {
100 int major = MAJOR(inode->i_rdev);
101 int minor = MINOR(inode->i_rdev);
102
103 printk("%s: major=%d, minor=%d\n", __func__, major, minor);
104
105 return 0;
106}
107
108static int demodrv_release(struct inode *inode, struct file *file)
109{
110 return 0;
111}
112
113static ssize_t
114demodrv_read(struct file *file, char __user *buf, size_t count, loff_t 
*ppos)
115{
116 size_t nbytes =
117 demodrv_read_write(buf, count, MYDEMO_READ);
118
119 printk("%s: read nbytes=%d done at pos=%d\n",
120 __func__, nbytes, (int)*ppos);
121
122 return nbytes;
123}
124
125static ssize_t
126demodrv_write(struct file *file, const char __user *buf, size_t count, 
loff_t *ppos)
127{
128
129 size_t nbytes =
130 demodrv_read_write((void *)buf, count, MYDEMO_WRITE);
131
132 printk("%s: write nbytes=%d done at pos=%d\n",
133 __func__, nbytes, (int)*ppos);
134
135 return nbytes;
136}
137
138static int
139demodrv_mmap(struct file *filp, struct vm_area_struct *vma)
140{
141 unsigned long pfn;
142 unsigned long offset = vma->vm_pgoff << PAGE_SHIFT;
143 unsigned long len = vma->vm_end - vma->vm_start;
144
145 if (offset >= MAX_DEVICE_BUFFER_SIZE)
146 return -EINVAL;
147 if (len > (MAX_DEVICE_BUFFER_SIZE - offset))
148 return -EINVAL;
149
150 printk("%s: mapping %ld bytes of device buffer at offset %ld\n",
151 __func__, len, offset);
152
153 /* pfn = page_to_pfn (virt_to_page (ramdisk + offset)); */
154 pfn = virt_to_phys(device_buffer + offset) >> PAGE_SHIFT;
155
156 if (remap_pfn_range(vma, vma->vm_start, pfn, len, 
vma->vm_page_prot))
157 return -EAGAIN;
158
159 return 0;
160}
161
162static long
163demodrv_unlocked_ioctl(struct file *filp, unsigned int cmd, unsigned long 
arg)
164{
165 unsigned long tbs = MAX_DEVICE_BUFFER_SIZE;
166 void __user *ioargp = (void __user *)arg;
167
168 switch (cmd) {
169 default:
170 return -EINVAL;
171
172 case MYDEV_CMD_GET_BUFSIZE:
173 if (copy_to_user(ioargp, &tbs, sizeof(tbs)))
174 return -EFAULT;
175 return 0;
176 }
177}
178
179static const struct file_operations demodrv_fops = {
180 .owner = THIS_MODULE,
181 .open = demodrv_open,
182 .release = demodrv_release,
183 .read = demodrv_read,
184 .write = demodrv_write,
185 .mmap = demodrv_mmap,
186 .unlocked_ioctl = demodrv_unlocked_ioctl,
187};
188
189static struct miscdevice mydemodrv_misc_device = {
190 .minor = MISC_DYNAMIC_MINOR,
191 .name = DEMO_NAME,
192 .fops = &demodrv_fops, 
193};
194
195static int __init simple_char_init(void)
196{
197 int ret;
198
199 device_buffer = kmalloc(MAX_DEVICE_BUFFER_SIZE, GFP_KERNEL);
200 if (!device_buffer)
201 return -ENOMEM;
202
203 ret = misc_register(&mydemodrv_misc_device);
204 if (ret) {
205 printk("failed register misc device\n");
206 kfree(device_buffer);
207 return ret;
208 }
209
210 mydemodrv_device = mydemodrv_misc_device.this_device;
211
212 printk("succeeded register char device: %s\n", DEMO_NAME);
213
214 return 0;
215}
216
217static void __exit simple_char_exit(void)
218{
219 printk("removing device\n");
220
221 kfree(device_buffer);
222 misc_deregister(&mydemodrv_misc_device);
223}
224
225module_init(simple_char_init);
226module_exit(simple_char_exit);
227
228MODULE_AUTHOR("Benshushu");
229MODULE_LICENSE("GPL v2");
230MODULE_DESCRIPTION("simpe character device");
```

​		本实验是在上一个实验基础上实现锁定（pin）用户内存，也就是另外一个方式来映射用户内存。实现锁定用户内存的主要函数就是 get_user_pages()函数。

​		get_user_pages()函数

​		用于把用户空间的虚拟内存空间传到内核空间，内核空间为其分配物理内存并建立相应的映射关系，实现过程如图所示。例如，在 camera 驱动的 V4L2 核心架构中可以使用用户空间内存类型（V4L2_MEMORY_USERPTR）来分配物理内存，其驱动的实现使用的是 get_user_pages()函数。

```
long get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start, unsigned long nr_pages, int write, int force, struct page **pages, struct vm_area_struct **vmas)
```

![image-20240924155030141](image/image-20240924155030141.png)

​		在本实验中，我们把 read 和 write 方法中的用户空间的 buffer 进行锁定（pin），然后重新分配物理内存，并且映射到用户空间的 buffer 中，整个过程有点类似 mmap方法。

​		我们来看本实验的 read 和 write 方法，分别调用了内部函数 demodrv_read_write()，该函数实现在第 24 行。

​		第 34 行，dev_buf 指向设备 buffer。

​		第 37 行，计算用户空间的 buffer 一共有多少页。

​		第 41 行，分配页数据结构 struct page，每一个页有一个页数据结构 struct page，这里 pages 可以看做是页数据结构 struct page 的数组，数组的成员就是页数据结构struct page 指针。

​		第 47 行，申请一个 mm->mmap_sem 的读者类型的信号量。

​		第 49 行，调用 get_user_pages_fast()函数来实现锁定任务，即 pin 住用户空间 buffer对应在内核态的物理内存，同时也完成新分配的物理内存和设备 buffer 之间的映射关系。

```
int get_user_pages_fast(unsigned long start, int nr_pages, int write,
struct page **pages)
```

get_user_pages_fast()函数有 4 个参数：

-  start：用户空间 buffer 的起始地址

-  nr_pages：需要锁定的页面的数量

-  write：这些页面是否需要可写

-  pages：当锁定完成之后，pages 指向已经锁定完成的物理页面。

​		第 55 行，释放读者类型的信号量。

​		第 59 行，对已经锁定的页面进行进一步处理。

​		第 60 行，kmap 函数是把物理页面 pages[i]进行一个临时的映射，从而得到一个内核态的虚拟地址 kmap_addr。

​		第 64 行，对于读操作，把设备 buffer 的内容拷贝到我们已经锁定好的物理页面。

​		第 69 行，对于写操作，把锁定好的物理页面的内容写入到设备 buffer 定义的页面。

​		第 77 行，这个步骤很重要，因为 get_user_pages_fast()函数之所以锁定，是因为

对物理页面进行了增加了页引用计数（get_page()），这里要释放页索引（put_page）。

关于页引用计数，读者可以阅读蓝色《奔跑吧 Linux 内核》第 2 章相关内容。

​		第 78 行，取消刚才建立的临时映射。

​		test_issue 测试代码如下。

```
1 #include <stdio.h>
2 #include <fcntl.h>
3 #include <unistd.h>
4 #include <sys/mman.h>
5 #include <string.h>
6 #include <errno.h>
7 #include <fcntl.h>
8 #include <sys/ioctl.h>
9 #include <malloc.h>
10
11#define DEMO_DEV_NAME "/dev/my_demo_dev"
12
13#define MYDEV_CMD_GET_BUFSIZE 1 /* defines our IOCTL cmd */
14
15int main()
16{
17 int fd;
18 int i;
19 size_t len;
20 char *read_buffer, *write_buffer;
21
22 fd = open(DEMO_DEV_NAME, O_RDWR);
23 if (fd < 0) {
24 printf("open device %s failded\n", DEMO_DEV_NAME);
25 return -1;
26 }
27
28 if (ioctl(fd, MYDEV_CMD_GET_BUFSIZE, &len) < 0) {
29 printf("ioctl fail\n");
30 goto open_fail;
31 }
32
33 printf("driver max buffer size=%d\n", len);
34
35 read_buffer = malloc(len);
36 if (!read_buffer)
37 goto open_fail;
38
39 write_buffer = malloc(len);
40 if (!write_buffer)
41 goto buffer_fail;
42
43 /* modify the write buffer */
44 for (i = 0; i < len; i++)
45 *(write_buffer + i) = 0x55;
46
47 if (write(fd, write_buffer, len) != len) {
48 printf("write fail\n");
49 goto rw_fail;
50 }
51
52 /* read the buffer back and compare with the mmap buffer*/
53 if (read(fd, read_buffer, len) != len) {
54 printf("read fail\n");
55 goto rw_fail;
56 }
57
58 if (memcmp(write_buffer, read_buffer, len)) {
59 printf("buffer compare fail\n");
60 goto rw_fail;
61 }
62
63 printf("data modify and compare succussful\n");
64
65 free(write_buffer);
66 free(read_buffer);
67 close(fd);
68
69 return 0;
70
71rw_fail:
72 if (write_buffer)
73 free(write_buffer);
74buffer_fail:
75 if (read_buffer)
76 free(read_buffer);
77open_fail:
78 close(fd);
79 return 0;
80}
```

​		本实验的测试代码不复杂，就是通过 write 函数把设备 buffer 进行改写，然后通过 read 函数把设备 buffer 又读回来，看看内容是否一致。

5．思考题提示

​		小明同学跑 test_issue 程序发现跑出错误了。“buffer compare fail”说明 buffer 读回来的数据和原始值不一样。而跑 test_ok 程序却是正确的。这是为什么呢？

​		小明同学对比了 test_issue 和 test_ok，最大的不同就是使用 malloc 来分配 userbuffer，而不是通过 mmap 来分配的匿名页面，那究竟是什么原因导致的呢？

​		这问题其实是笨叔在实际项目开发中遇到的，因此抽象出来把它变成一个实验。这个思考题对初学者来说有不小的难度。若对该问题的来龙去脉想明白，那么对 Linux的内存管理的理解会上一个台阶。

​		遇到这种问题，我们最简单最粗暴也是最有效的调试办法就是把 buffer 打印出来看看。在内核里，可以使用 print_hex_dump_bytes()函数。比如在 demodrv_read_write()函数的第 61,67,72 行添加打印语句。

![image-20240924155404256](image/image-20240924155404256.png)

运行 test_issue 程序，打印结果如下：

![image-20240924155410523](image/image-20240924155410523.png)

​		发现 pin 的 page，开头的数据都是 0，而不是 0x55，这是为什么呢? 理论上这个page 应该全部都是 0x55 才对。

​		我们在运行 test_ok 程序，发现 pin 的 page 的数据，从开头到介绍都是 0x55，这就奇怪了?

![image-20240924155420001](image/image-20240924155420001.png)

​		我们需要进一步思考 malloc 和 mmap 来分配虚拟内存，究竟有什么不一样。

​		我们继续在 demodrv_read_write()函数里添加打印，这次我们把用户空间 buffer 的起始地址打印出来。

​		运行 test_issue 程序，打印的用户空间 buffer 起始地址为 buf=0x0008c0d8。

![image-20240924155436547](image/image-20240924155436547.png)

​		运行 test_ok 程序，打印的用户空间 buffer 起始地址为 0xb6f99000。

![image-20240924155446703](image/image-20240924155446703.png)

​		我们发现问题，使用 malloc 分配 4KB 的虚拟内存，起始地址为 0x0008c0d8，而使用 mmap 分配的 4KB 虚拟内存，起始地址是 0xb6f99000。前者的起始地址没有页面对齐（4KB），后者以 4KB 页面对齐。

​		这就是为什么使用 malloc 分配内存的 test_issue 程序，在内核空间中看到 pin 住的物理页面有一部分地址的内容是 0x0，而另外一部分地址的内容是 0x55。